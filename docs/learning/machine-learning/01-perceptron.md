<h1 style="text-align:center">感知机</h1>

感知机是二分类的线性分类模型，属于判别模型。训练基于误分类的目标函数，利用梯度下降法进行优化。

## 理论

### 问题

对于线性可分数据集，求一个超平面将数据划分为两部分。

![数据集](../../image/machine-learning/01-dataset.png)

### 模型

线性模型加符号函数，将实数域转化为二分类。

(1) 原始形式

$$
    f(x)=sign(w \cdot x + b)
$$

(2) 对偶形式

$$
    f(x)=sign(\sum_{j=1}^{N} {\alpha_j}{y_j}{x_j} \cdot x + b)
$$

### 训练

当一个实例点被误分类，即位于分离超平面的错误一侧时，则调整$w,b$的值，使分离超平面向该误分类点的一侧移动，以减少该误分类点与超平面的距离，直到超平面越过该误分类点使其被正确分类。

**(1) 原始形式**

- 目标函数

$$
    L(w,b)=-\sum_{x_i \isin M}y_{i}(w \cdot x_{i} + b)
$$
其中$M$为误分类点的集合。

- 学习算法

输入：训练数据集$T=\{(x_1,y_1),(x_2,y_2),\cdot\cdot\cdot(x_N,y_N)\}$

输出：$w,b$;感知机模型$f(x)=sign(w \cdot x + b)$

1. 选取初值$w_0,b_0$

2. 在训练数据中选取数据$(x_i,y_i)$

3. 如果$y_i(w \cdot x_i + b) \leq 0$,

$$
    w \gets w + \eta{y_i}{x_i}
    \\
    b \gets b + \eta{y_i}
$$

4. 转至2，直到训练集中没有误分类点

**(2) 对偶形式**

将$w$和$b$表示为实例$x_i$和标记$y_i$的线性组合的形式，通过求解其系数得$w$和$b$。

$$
    w = \sum_{i=1}^{N}{\alpha_i}{y_i}{x_i}
    \\
    b = \sum_{i=1}^{N}{\alpha_i}{y_i}
    \\
    \alpha_i = n_{i}\eta
$$

- 目标函数

$$
    L(w,b)=-\sum_{x_i \isin M}y_{i}(\sum_{j=1}^{N} {\alpha_j}{y_j}{x_j} \cdot x + b)
$$
其中$M$为误分类点的集合。

- 学习算法

输入：训练数据集$T=\{(x_1,y_1),(x_2,y_2),\cdot\cdot\cdot(x_N,y_N)\}$

输出：$\alpha,b$;感知机模型$f(x)=sign(\sum_{j=1}^{N} {\alpha_j}{y_j}{x_j} \cdot x + b)$

1. 选取初值$\alpha \gets 0, b \gets 0$

2. 在训练数据中选取数据$(x_i,y_i)$

3. 如果$y_i(\sum_{j=1}^{N} {\alpha_j}{y_j}{x_j} \cdot x + b) \leq 0$,

$$
    n_i \gets n_i + 1
    \\
    \alpha_i \gets \alpha_i + \eta
    \\
    b \gets b + \eta{y_i}
$$

4. 转至2，直到训练集中没有误分类点

## 面试

(1) 单层感知机无法解决异或问题，那多层感知机在没有sigmoid函数的情况下，能解决异或问题吗

> 可以

(2) 既然多层感知机用sign能解决非线性问题，那为啥要sigmoid函数？

> 激活函数需要满足的条件：
> 
> 1)非线性，处处可导
> 
> 2)导数尽量简单
> 
> 3)导数值域范围固定，以免梯度消失或爆炸
> 
> 所以sigmoid比sign效果更好

(3) 感知机时二分类的线性模型，和多分类、多标签分类有什么区别

> 分类问题：在监督学习中，输出变量取有限个离散值
> 
> 二分类：分类的类别为两个
> 
> 多分类：分类的类别为多个
> 
> 多标签：一个样本存在多个类别

(4) 感知机和SVM的区别

> SVM的基本模型时定义在特征空间上间隔最大的线性分类器，间隔最大使其有别于感知机

## 实践


## 参考

[1] [统计学习方法](#参考)

[2] [感知机算法面试问题汇总](https://blog.csdn.net/longshaonihaoa/article/details/107234217)
